\documentclass[oneside]{article}
\usepackage{fullpage}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.png,.pdf}
\usepackage{hyperref}
\usepackage[format=plain,font=small]{caption}
\usepackage[small]{titlesec}
\usepackage[round,sectionbib]{natbib}
\usepackage{pdfpages}
\bibliographystyle{plainnat}
\renewcommand\rmdefault{bch}
\linespread{1.07}

\title{Tidy data}
\author{Hadley Wickham}

\begin{document}
\maketitle

\section{Introduction}

Data cleaning is a vital first step in any analysis, and is often repeated multiple times as new problems come to light. Data cleaning encompasses many activities from outlier checking to missing value imputation. This paper focusses on a small subset of data cleaning that I call data tidying: getting the data in the right format for  manipulation, modelling, and visualisations. This paper defines tidy data, and shows how tidyness makes data easier to work. 

The principles of tidy data are inspired by databases and Codd's relational algebra, but the set algebra that powers SQL and relational databases is not good a good fit for most statistical problems. It has been driven by my work in data analysis, where I have struggled with many datasets organised in bizarre ways. It grows out of my work on the plyr \citep{me:plyr} and reshape \citep{wickham:2007b} packages in R.

My definition of tidy data is not exhaustive, and it focusses on the type of data that statisticians most commonly encounter: rectangular data defined by rows and columns. It is not a good fit for data on networks, \ldots, or \ldots Additional, because the focus of this definition is on explicit definition of the data, it may not be suitable for very large datasets - these may require special infrastructure for performance.

Defining tidy data is important because getting data into the right form for analysis is very important, but there are few existing resources that rigorously define what tidy data is and how to tidy up messy data. A good definition of tidy data also makes messy data easier to work with by showing exactly why it feels dirty and how it can be tidied up. This paper will make life easier for:

\begin{itemize}

\item Applied data scientists, because it will help guide the collection and storage of data.

\item Teachers, by making one part of data cleaning easier to teach by providing a firm theoretical foundation.

\item Data analysts: because you can focus on the problem you are trying to solve, instead of data wrangling.

\end{itemize}

The paper is divided into three sections:

\begin{itemize}

\item Defining tidy data: what are the important characteristics of tidy data, and how can you identify the key components of tidy data from a messy data set.

\item Tidying dirty data: most real world data is not tidy, and in this section what operations are needed to make messy data tidy. These techniques will be illustrated by some of the messy datasets I've encountered in the course of an analysis.

\item Working with tidy data: I'll show how tidy data is easy to model, transform and visualise, and how the tidy data, plus tidy tools, makes previously difficult problems easy. Tidy tools tidy data as input and return tidy data as output, ensuring that data stays tidy throughout the entire analysis.

\end{itemize}

I will illustrate these ideas using R, because that is where I have developed the computational tools to support tidy data, but I believe the principles apply to any programming language which deals with data. In R, the majority built in modelling tools work best with tidy data. The plyr and ggplot2 \citep{me:ggplot2} packages work well with tidy data, and the reshape2 package provides tools useful for making dirty data tidy. The principles of tidy data will also help us criticise existing R functions, and I will show how untidy tools make more work for the analyst.

\section{Defining tidy data}

``Happy families are all alike; every unhappy family is unhappy in its own way.'' ----Leo Tolstoy

Tidy datasets are all alike; every messy dataset is messy in its own way. Here we will define the essence of tidy data, and provide important vocabulary for describing data sets.

Statistical data usually comes in a rectangular \textbf{table}, made up of rows and columns. It is usually labelled with column names, and sometimes with row names. Such data is a collection of \textbf{value}s, each either a single number (if quantitative) or a single string (if qualitative). Values are grouped into \textbf{variables}, measurements of a single attribute. Multiple measurements made on the same \textbf{experimental unit} form an \textbf{observation}. 

The following table illustrates a common data format. There are two columns and two rows (plus headers), defining three variables (sex, pregnancy status, count) and four observations.

\begin{verbatim}
         | Pregnant  Not-Pregnant
  -------+-----------------------
  Male   |        0             5
  Female |        1             4
\end{verbatim}

Data is messy or tidy depending on how these structures are arranged. In tidy data each variable lies in a column, and each observation lies in a row. This form is also sometimes called long-form data. We will call all other arrangements messy.  An additional restriction is that each table should store data about one type of experimental unit.

The following table shows the tidy representation of the previous data:

\begin{verbatim}
  pregnant  sex    | n
  -----------------+-
  No        Female | 4
  No        Male   | 5
  Yes       Female | 1
  Yes       Male   | 0
\end{verbatim}

While order of variables and observations does not affect analysis, a good ordering makes it easier to scan the raw data. Variables can be ordered by grouping them into identifier and measured variables. \textbf{Identifier variables} describe the experimental design and are known in advance (in a sense they're not really measurements). \textbf{Measured variables} are what we actually measure in the study. Identifier variables should come first first, ordered in terms of their natural hierarchy if present, otherwise alphabetically. Measured variables come next, ordered alphabetically. Rows can then be ordered so that the first id variable varies slowest, followed by the second, and so on.

The following sections describe how to make messy data tidy, and then justify this definition of tidy data by showing how easier it is to work with.

\section{Tidying messy data}

Messy data can be messy in any number of ways - it just needs to violate the restriction of variables in columns and observations in rows. Real data violates these restrictions in almost every way imaginable. In this section, I will focus on some of the most common problems, illustrated with specific datasets that I have worked with:

\begin{itemize}

\item column headers are values, not variable names (pew religion, billboard)
\item multiple variables are stored in one column (tb)
\item variables are stored in both rows and columns (weather)
\item multiple types of experimental unit stored in the same table
\item experimental unit stored in the multiple tables
\end{itemize}

Messy data, including types of messiness not explicitly described above, can be tidyed in a surprisingly small set of tools: melting, string splitting, and less often cast. The following sections discuss each type of problem in turn, introducing the tools needed to tidy it.

The first step in any tidying is to identify the variables and observations.

\subsection{Column headers are values, not variables names}

A common case of messy data is tabular data designed for presentation, where variables form both the rows and columns, and column headers are values, not variable names. While I call this type of data dirty, in some cases it can be extremely useful. It provides efficient storage for completely crossed designs, and it can provide extremely efficient computation if desired operations can be expressed as matrix operations. It is also necessary for many multivariate methods whose underlying theory is based on manipulation of matrices. But it doesn't generalise well, requiring extensions into higher dimensions to deal with more variables, and can be very inefficient if there are a lot of missing values or variables are nested, not crossed. This makes it inappropriate as a general storage format.

\begin{verbatim}
Pew religion-income data
\end{verbatim}

Another common use of this data is for recording regularly spaced observations over time. For example, the billboard dataset records the date a song first entered the billboard top 100. To record its ranking in the following weeks, 75 variables, from week1 to week75, are needed.  This form of storage is useful for data entry because it reduces duplication - otherwise each song in each week would need its own row.

\begin{verbatim}
Billboard data
\end{verbatim}

This type of data is so common that in R a number of graphical tools have been designed specifically to visualise it, e.g. \texttt{barplot}, \texttt{matplot}, \texttt{dotchart}, \texttt{mosaicplot}. Other tools are designed to provide common manipulations like \texttt{sweep}, \texttt{prop.table} and \texttt{margin.table}. Other tools can produce this type of output from formerly tidy data: \texttt{table}, \texttt{xtabs}, \texttt{tapply}. The final section of the paper will alternative tools that work with tidy data, and provide more flexibility for an analysis.

To tidy up this type of data we need to melt, or stack it, turning columns into rows. Melting is parameterised by a list of variables to keep in columns (cvars), or equivalently a list of columns that should be turned into rows (rvars). It works by adding a new indicator variable to the dataset and stacking the repeated cvars.

\begin{verbatim}
tidyed pew data
tidyed billboard data
\end{verbatim}

\subsection{Multiple variables stored in one column}

After melting, it often happens that the indicator variable actually records information about more than on variable. This is illustrated by the tb dataset - each column represents two values: age and sex.  

\begin{verbatim}
Original tb
\end{verbatim}

After melting the data to combine variables spread across columns, we get

\begin{verbatim}
Molten tb
\end{verbatim}

Typically, column headers in this format are separated by some character (e.g. \texttt{.}, \texttt{-}, \texttt{\_}, \texttt{:}) and we can simply split the string up based on that character.  In other cases, such as for this tb data, more careful string processing is required, or the variable names can be matched to a hand made table that converts compound values to multiple variables.

Doing this for the tb data yields:

\begin{verbatim}
tidy tb
\end{verbatim}

Storing the data in this form resolves another problem in the original data: it's more informative to compare tb rates across countries, rather than counts, but to compute this we need to know the population of each category. In the original data format, there is no easy way to add this information, it had to be stored in separate file and was difficult to correctly match up to the counts. In tidy form, adding populations (and rates) are easier: they just become additional columns.

\subsection{Variables are stored in both rows and columns}

The most complicated form of dirty data is when variables have been stored in both rows and columns. The weather data below has variables in single columns (id, year, month), spread across columns (day) and in rows (TMIN, TMAX).  

\begin{verbatim}
Original weather data
\end{verbatim}

Melting the data gets us to:

\begin{verbatim}
Molten weather data
\end{verbatim}

Which is mostly tidy, but we have one variable stored in rows: the observation type. Fixing this requires the cast, or unstack, operation, which performs the inverse of melting, and rotates the \texttt{obs} variable back out into the columns to get:

\begin{verbatim}
tidy weather data
\end{verbatim}

\section{Working with tidy data}

tidy data is only worthwhile in so much as it makes the rest of the analysis easier. This section discusses general tools for working with tidy data, organised into three components of data analysis: transformation, modelling and visualisation.

\subsection{Modelling}


Modelling is the inspiration that has driven most of this work. Every statistical language has a way of describing a model as a connection between different variables:

\begin{itemize}

\item R:
\item SAS:
\item SPSS:
\end{itemize}

Some have also tried to adapt these methods for other wide data. Repeated measures in SAS.

\subsection{Transformation}

Here I define transformation very generally - any operation that takes the data as input and returns modified data. It includes variable-by-variable transformation (e.g. \texttt{log} or \texttt{sqrt}), as well as aggregation, filtering and reordering. In my experience there are four extremely common operations that are performed over and over again in the course of an analysis.  These are the four fundamental verbs of data manipulation:

\begin{itemize}

\item filtering, or subsetting, where some observations are removed
\item aggregation, where multiple values are collapsed into a single value
\item reordering, where the order of observations is changed
\item mutation, where new variables are added or existing variables modified 
\end{itemize}

Each operation takes a data frame and some parameters as input and returns a data frame as output - this makes them easily composable to solve more difficult problems.  

These four verbs are often modified by the \textbf{by} preposition. Many times we need group-wise aggregates, transformations or subsets: pick the biggest in each group, average over replicates, \ldots

Some aggregations occur so frequently they deserve their own optimised implementations. An example of one such operation is (weighted) counting, which occurs so often that optimisation is worthwhile. Any such operator

\subsection{Tidy tools}

These need to return data in long form, so that they can easily be combined with the unaggregated data with join. The table function in base R does not do this - it returns a vector/matrix/array with names. This is problematic because it forces crossing of variables, so all combinations must be counted (many 0 zeros for combinations that don't exist in data), and it's difficult to combine back with the original data.

\textbf{Multiple datasets}

This raises another important issue - when doing data analysis we don't usually work with a single set of data, but need to be able to fluidly combine multiple data sets, whether they are related data or the same data at multiple levels of aggregation. We need two additional operators for dealing with multiple datasets:

\begin{itemize}

\item \texttt{join}: An advantage of long form data is the ease with which it can be combined: we just need a join operator which works by matching common variables and adding new columns. Compare this to the difficulty of combining wide datasets stored in arrays - these typically require painstaking alignment before matrix operations can be used, and errors are very hard to detect. 

\item \texttt{match\_df}: Another common operation is subsetting individual level data based on some summary level statistic. This is accomplished using a function very similar to join that only returns matching rows instead of trying to combine the columns.

\end{itemize}

These ideas have developed iteratively - as my understanding of tidy data improved, so to did these tools, which made me realise understand better what tools tidy data needs.

\textbf{Case study}

The following examples illustrates these ideas.  

In this snippet of code, we first count the number of deaths in each hour of the day (\texttt{hod}) for each cause of death (\texttt{cod}), then remove missing hours, join on fuller descriptions of the disease codes, and transform to compute proportions within a disease.

\begin{verbatim}
hod2 <- count(deaths, c("hod", "cod"))
hod2 <- subset(hod2, !is.na(hod))
hod2 <- join(hod2, codes)
hod2 <- ddply(hod2, "cod", transform, prop = freq / sum(freq))

# Idea for new syntax

deaths &
  count(c("hod", "cod")) &
  subset(!is.na(hod)) &
  join(codes) &
  by("cod", transform, prop = freq / sum(freq))
\end{verbatim}

In the following code, for each disease we calculate the deviation from a uniform pattern over time, find unusual diseases, and then go back to extract matching records from the original data.

\begin{verbatim}
uni_dist <- function(x) sqrt(mean((x - 1/23) ^ 2))

devi <- ddply(hod2, "cod", summarise, n = sum(freq), 
  dist = uni_dist(prop))
unusual <- subset(devi, n > 1200 & dist >= 0.009)
hod_unusual <- match_df(hod2, unusual)

devi <- by("cod", summarise, n = sum(freq), dist = uni_dist(prop))
deaths & match(devi & subset(n > 1200 & dist >= 0.009))
\end{verbatim}

\subsection{Visualisation}

Lattice graphics tries to stick too closely to the modelling structure.

ggplot2: map variables to different 

\section{Conclusion}

% bibtool -x tidy-data.aux -c -o references.bib 
\bibliography{references}

\end{document}
